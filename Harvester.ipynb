{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Journal Data Harvester\n",
    "\n",
    "## v. 1.1\n",
    "\n",
    "1. Grabs all DOI associated with `ISSN` set below from CR, saves to DOI file\n",
    "1. Attempts to extract all metadata from CR for those DOI for first stage of data set completion. Will save to CSV when completed\n",
    "1. Attempts to download the HTML landing pages from resolving `http://dx.doi.org/doi`\n",
    "1. Attempts to screen scrape the Abstract from that page, and rebuild an updated CSV file with that info\n",
    "\n",
    "## Caveates\n",
    "- Beautiful Soup grab is domain specific, Soup Find pattern set in first cell\n",
    "\n",
    "\n",
    "## Set the values in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOURNAl TO Grab\n",
    "\n",
    "#Name of the folder and prefix for our file names\n",
    "TITLE = \"SMR_test\"\n",
    "\n",
    "#Will be term for CrossRef ISSN search\n",
    "ISSN = \"1441-3523\"\n",
    "\n",
    "\n",
    "#Soup Pattern\n",
    "#This is the value passed to Beautiful Soup to grab Abstract Text. Different for each Domain\n",
    "\n",
    "# T & F - soup.find(\"div\",{\"class\":\"abstractSection abstractInFull\"}).text\n",
    "SOUP_TAG = \"div\"\n",
    "SOUP_DICT = {\"class\":\"abstractSection abstractInFull\"}\n",
    "\n",
    "\n",
    "# Human Kinetics - soup.find(\"section\",{\"class\": \"abstract\"}).text\n",
    "#SOUP_TAG = \"section\"\n",
    "#SOUP_DICT = {\"class\": \"abstract\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAGS - For bedbugging mostly\n",
    "\n",
    "#Set to Zero to download everything\n",
    "#(At least 50 to make sure habanero pagers works. v 2 will fix this)\n",
    "SUBSET_SIZE = 50\n",
    "\n",
    "DO_STAGE_1 = True\n",
    "DO_STAGE_2 = True\n",
    "DO_STAGE_3 = True\n",
    "DO_STAGE_4 = True\n",
    "\n",
    "#What to name our columns\n",
    "ART_COLUMNS = [\n",
    "    \"DOI\",\n",
    "    \"AUTHOR_NAME\",\n",
    "    \"AUTHOR_ORDER\",\n",
    "    \"AFFILIATION\",\n",
    "    \"TITLE\",\n",
    "    \"DATE\",\n",
    "    \"VOLUME\",\n",
    "    \"ISSUE\",\n",
    "    \"PAGES\",\n",
    "    \"KEYWORDS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prep Done.\n",
      "\n",
      "STAGE 1: Harvesting DOIs for this title from CR\n",
      "Cross Ref has this many DOIs:  871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DOIs from CR for SMR_test :  60\n",
      "...Done.\n",
      "\n",
      "STAGE 2: Harvest Metadata for each article and put into Dataframe\n",
      "Building Dataframe...\n",
      "50\n",
      "Number of Problem DOIS:  8\n",
      "...Done.\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "# Test to see if habanero is installed / available\n",
    "\n",
    "import IPython\n",
    "\n",
    "try:\n",
    "    from habanero import Crossref\n",
    "except:\n",
    "    \n",
    "    print(\"*******************************\")\n",
    "    print(\"Need to install habanero still!\")\n",
    "    print(\"Installation will proceed. When completed you'll need to\")\n",
    "    print(\"Re-run whole notebook again\")\n",
    "    print(\"*******************************\")\n",
    "    %pip install habanero\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "#libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from habanero import Crossref\n",
    "import glob\n",
    "\n",
    "#make folders\n",
    "try:\n",
    "    os.mkdir(TITLE)\n",
    "except:\n",
    "    print(\"Folder already made\")\n",
    "    \n",
    "print(\"Prep Done.\")\n",
    "\n",
    "\n",
    "####VARIABLES\n",
    "cr = Crossref()\n",
    "\n",
    "\n",
    "if DO_STAGE_1:\n",
    "\n",
    "    #### Stage 1\n",
    "    print(\"\\nSTAGE 1: Harvesting DOIs for this title from CR\")\n",
    "    try:\n",
    "        j_doi_count = cr.journals(ISSN)\n",
    "        max_dois = int(j_doi_count['message']['counts']['total-dois'])\n",
    "        print(\"Cross Ref has this many DOIs: \",str(max_dois))\n",
    "    except:\n",
    "        print(\"CrossRef API is having troubles... Couldn't find number of DOIs associated with title\")\n",
    "\n",
    "\n",
    "    if SUBSET_SIZE != 0:\n",
    "        max_dois = SUBSET_SIZE\n",
    "\n",
    "    #Harvest all DOIs for this journal\n",
    "    try:\n",
    "        res = cr.journals(ids = ISSN, works = True, cursor = \"*\", cursor_max = max_dois, progress_bar = True)\n",
    "        sum([ len(z['message']['items']) for z in res ])\n",
    "        items = [ z['message']['items'] for z in res ]\n",
    "        items = [ item for sublist in items for item in sublist ]\n",
    "    except:\n",
    "        print(\"CrossRef API is having troubles... Couldn't harvest DOIs of title\")\n",
    "\n",
    "    print(\"Total DOIs from CR for\",TITLE,\": \",len(items))\n",
    "    with open(TITLE+\"/\"+ISSN+\"_DOI.txt\", \"w\") as doi_file:\n",
    "        for a in items:\n",
    "            #print(a['DOI'])\n",
    "            doi_file.write(a[\"DOI\"]+\"\\n\")\n",
    "\n",
    "    print(\"...Done.\")\n",
    "    \n",
    "if DO_STAGE_2:\n",
    "\n",
    "    #### Stage 2\n",
    "    print(\"\\nSTAGE 2: Harvest Metadata for each article and put into Dataframe\")\n",
    "\n",
    "    article_list = []\n",
    "    progress = 0\n",
    "    problem_DOIs = []\n",
    "\n",
    "    print(\"Building Dataframe...\")\n",
    "    with open(TITLE+\"/\"+ISSN+\"_DOI.txt\") as d_file:\n",
    "\n",
    "        for doi in d_file.readlines():\n",
    "            #progress bar\n",
    "            progress +=1\n",
    "\n",
    "            art = cr.works(doi)\n",
    "            a_title = art['message']['title'][0]\n",
    "\n",
    "            try:\n",
    "                a_vol = art['message']['volume']\n",
    "            except:\n",
    "                a_vol = \"NA\"\n",
    "\n",
    "            try:\n",
    "                a_issue = art['message']['issue']\n",
    "            except:\n",
    "                a_issue = \"NA\"\n",
    "             \n",
    "            try:\n",
    "                a_pages = art['message']['page']\n",
    "            except:\n",
    "                a_pages = \"NA\"\n",
    "\n",
    "            try:\n",
    "                a_date = str(art['message']['published']['date-parts'][0][0]) + \"-\" + \\\n",
    "                         str(art['message']['published']['date-parts'][0][1]) +\"-\"+ \\\n",
    "                         str(art['message']['published']['date-parts'][0][2])\n",
    "            except:\n",
    "                a_date = \"NA\"\n",
    "\n",
    "\n",
    "            try:    \n",
    "                a_kws = ', '.join(art['message']['subject'])\n",
    "\n",
    "            except:\n",
    "                a_kws = \"NA\"\n",
    "\n",
    "\n",
    "            try:\n",
    "                for author in art['message']['author']:\n",
    "                    a_dets = []\n",
    "                    a_dets.append(doi.strip(\"\\n\"))\n",
    "                    a_dets.append(author['given'] + \" \" + author['family'])\n",
    "                    a_dets.append(author['sequence'])\n",
    "                    a_dets.append(author['affiliation'][0]['name'])\n",
    "                    a_dets.append(a_title)\n",
    "                    a_dets.append(a_date)\n",
    "                    a_dets.append(a_vol)\n",
    "                    a_dets.append(a_issue)\n",
    "                    a_dets.append(a_pages)\n",
    "                    a_dets.append(a_kws)\n",
    "                    article_list.append(a_dets)\n",
    "            except:\n",
    "                problem_DOIs.append(doi)\n",
    "\n",
    "            if progress % 50 == 0:\n",
    "                print(progress)\n",
    "                \n",
    "    j_data = pd.DataFrame(article_list)\n",
    "    j_data.columns = ART_COLUMNS\n",
    "\n",
    "    print(\"Number of Problem DOIS: \", len(problem_DOIs))\n",
    "\n",
    "    with open(TITLE+\"/\"+ISSN+\"_BAD_DOI.txt\",\"w\") as b_doi:\n",
    "        for d in problem_DOIs:\n",
    "            b_doi.write(\"https://dx.doi.org/\"+d.strip(\"\\n\")+\", couldn't get all metadata (stage 2)\\n\")\n",
    "\n",
    "    #CSV of progress thus far\n",
    "    j_data.to_csv(TITLE+\"/\"+TITLE+\"_\"+ISSN+\".csv\",index=False)\n",
    "    print(\"...Done.\")\n",
    "    \n",
    "if DO_STAGE_3:\n",
    "    \n",
    "    #### Stage 3\n",
    "    print(\"\\nSTAGE 3: Download HTML Landing Pages\")\n",
    "    \n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0'\n",
    "    }\n",
    "    \n",
    "    counter = 0\n",
    "    with open(TITLE+\"/\"+ISSN+\"_DOI.txt\") as doi_file:\n",
    "        for d in doi_file:\n",
    "            counter += 1\n",
    "            url_to_grab = \"http://dx.doi.org/\"+d.strip(\"\\n\")\n",
    "            label = d.replace(\"/\",\"_\")\n",
    "            html = requests.get(url_to_grab,headers=headers).text\n",
    "            if not glob.glob(TITLE+\"/\"+label+\".html\"): #Only download if we didn't already\n",
    "                try:\n",
    "                    html_grabbed = open(TITLE+\"/\"+label+\".html\",\"w\")\n",
    "                    html_grabbed.write(html)\n",
    "                except:\n",
    "                    print(\"Problem downloading: \",d)\n",
    "            if counter % 50 == 0:\n",
    "                print(counter)\n",
    "\n",
    "    print(\"...Done.\")\n",
    "    \n",
    "    \n",
    "if DO_STAGE_4:\n",
    "\n",
    "    #The j_data dataframe won't be here unless stage two has been completed\n",
    "    #put it together again if so\n",
    "    if DO_STAGE_2 == False:\n",
    "        jdata = pd.read_csv(TITLE+\"/\"+TITLE+\"_\"+ISSN+\".csv\")\n",
    "\n",
    "    #### Stage 4\n",
    "    print(\"\\nSTAGE 4: Enrich Dataframe with HTML Data\")\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    art_list_md = []\n",
    "    trouble_html = []\n",
    "    counter = 0\n",
    "\n",
    "    for html_md in glob.glob(TITLE+\"/*.html\"):\n",
    "        counter +=1\n",
    "        with open(html_md) as h_file:\n",
    "\n",
    "            a_md = []\n",
    "\n",
    "            try:\n",
    "                a_doi = html_md.split(\"/\")[1].replace(\"_\",\"/\")[:-5].replace(\"\\n\",\"\")\n",
    "            except:\n",
    "                a_doi = \"NA\"\n",
    "                trouble_html.append(html_md.replace(\"\\n\",\"\") + \", could not find DOI\\n\")\n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(h_file,'html.parser')\n",
    "                a_abs = soup.find(SOUP_TAG,SOUP_DICT).text\n",
    "            except:\n",
    "                trouble_html.append(html_md.replace(\"\\n\",\"\") + \", could not find Abstract\\n\")\n",
    "                a_abs = \"NA\"\n",
    "\n",
    "            a_md.append(a_doi)\n",
    "            a_md.append(a_abs)\n",
    "\n",
    "        art_list_md.append(a_md)\n",
    "        if counter % 50 == 0:\n",
    "            print(counter)\n",
    "\n",
    "    with open(TITLE+\"/\"+ISSN+\"_bad_html.txt\",\"w\") as b_html_file:\n",
    "        for h in trouble_html:\n",
    "            b_html_file.write(h)\n",
    "\n",
    "    j_extra_md = pd.DataFrame(art_list_md)\n",
    "    j_extra_md.columns = [\"DOI\",\"ABSTRACT\"]\n",
    "\n",
    "    final_df = j_data.merge(j_extra_md, left_on=\"DOI\",right_on=\"DOI\")\n",
    "    final_df.to_csv(TITLE+\"/\"+TITLE+\"_\"+ISSN+\".csv\",index=False)\n",
    "\n",
    "    print(\"...Done.\")\n",
    "    \n",
    "    \n",
    "print(\"All Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
