{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Journal Data Harvester\n",
    "\n",
    "CrossRef To T & F Version\n",
    "\n",
    "|Title|ISSN|Platform|\n",
    "|---|---|---|\n",
    "| _Journal of Sport Management_ | 0888-4773 |Human Kinetics & Elsvier|\n",
    "| _European Sport Management Quarterly_ | 1618-4742 |T&F|\n",
    "| _Sport Management Review_ | 1441-3523 |T&F|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOURNAl TO Grab\n",
    "\n",
    "TITLE = \"ESMQ\"\n",
    "ISSN = \"1618-4742\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: habanero in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (1.2.2)\n",
      "Requirement already satisfied: requests>=2.7.0 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from habanero) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from habanero) (4.55.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->habanero) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->habanero) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->habanero) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tim/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->habanero) (2019.9.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#This Library isn't in the core Python Libraries.\n",
    "# You need to run this cell and then restart the runtime to have it function.\n",
    "%pip install habanero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from habanero import Crossref\n",
    "import glob\n",
    "\n",
    "#make folders\n",
    "try:\n",
    "    os.mkdir(TITLE)\n",
    "except:\n",
    "    print(\"Folder already made\")\n",
    "    \n",
    "print(\"Prep Done.\")\n",
    "\n",
    "\n",
    "####VARIABLES\n",
    "cr = Crossref()\n",
    "\n",
    "\n",
    "#### Stage 1\n",
    "print(\"\\nSTAGE 1: Harvesting DOIs for this title from CR\")\n",
    "try:\n",
    "    j_doi_count = cr.journals(ISSN)\n",
    "    max_dois = int(j_doi_count['message']['counts']['total-dois'])\n",
    "    print(\"Cross Ref has this many DOIs: \",str(max_dois))\n",
    "except:\n",
    "    print(\"CrossRef API is having troubles... Couldn't find number of DOIs associated with title\")\n",
    "    \n",
    "#Harvest all DOIs for this journal\n",
    "try:\n",
    "    res = cr.journals(ids = ISSN, works = True, cursor = \"*\", cursor_max = max_dois, progress_bar = True)\n",
    "    sum([ len(z['message']['items']) for z in res ])\n",
    "    items = [ z['message']['items'] for z in res ]\n",
    "    items = [ item for sublist in items for item in sublist ]\n",
    "except:\n",
    "    print(\"CrossRef API is having troubles... Couldn't harvest DOIs of title\")\n",
    "    \n",
    "print(\"Total DOIs from CR for\",TITLE,\": \",len(items))\n",
    "doi_file = open(TITLE+\"/\"+ISSN+\"_DOI.txt\", \"w\")\n",
    "for a in items:\n",
    "    doi_file.write(a[\"DOI\"]+\"\\n\")\n",
    "\n",
    "print(\"...Done.\")\n",
    "\n",
    "\n",
    "#### Stage 2\n",
    "print(\"\\nSTAGE 2: Harvest Metadata for each article and put into Dataframe\")\n",
    "\n",
    "article_list = []\n",
    "progress = 0\n",
    "problem_DOIs = []\n",
    "\n",
    "print(\"Building Dataframe...\")\n",
    "with open(TITLE+\"/\"+ISSN+\"_DOI.txt\") as d_file:\n",
    "    \n",
    "    for doi in d_file.readlines():\n",
    "        #progress bar\n",
    "        progress +=1\n",
    "        \n",
    "        art = cr.works(doi)\n",
    "        a_title = art['message']['title'][0]\n",
    "        \n",
    "        try:\n",
    "            a_pages = art['message']['page']\n",
    "        except:\n",
    "            a_pages = \"NA\"\n",
    "        \n",
    "        try:\n",
    "            a_vol = art['message']['volume']\n",
    "        except:\n",
    "            a_vol = \"NA\"\n",
    "        try:\n",
    "            a_issue = art['message']['issue']\n",
    "        except:\n",
    "            a_issue = \"NA\"\n",
    "        \n",
    "        try:\n",
    "            a_date = str(art['message']['published-online']['date-parts'][0][0]) + \"-\" + \\\n",
    "                     str(art['message']['published-online']['date-parts'][0][1]) +\"-\"+ \\\n",
    "                     str(art['message']['published-online']['date-parts'][0][2])\n",
    "        except:\n",
    "            a_date = \"NA\"\n",
    "    \n",
    "        \n",
    "        try:    \n",
    "            a_kws = ', '.join(art['message']['subject'])\n",
    "\n",
    "        except:\n",
    "            a_kws = \"NA\"\n",
    "            \n",
    "    \n",
    "    \n",
    "        try:\n",
    "            for author in art['message']['author']:\n",
    "                a_dets = []\n",
    "                a_dets.append(doi)\n",
    "                a_dets.append(author['given'] + \" \" + author['family'])\n",
    "                a_dets.append(author['sequence'])\n",
    "                a_dets.append(author['affiliation'][0]['name'])\n",
    "                a_dets.append(a_title)\n",
    "                a_dets.append(a_date)\n",
    "                a_dets.append(a_vol)\n",
    "                a_dets.append(a_issue)\n",
    "                a_dets.append(a_pages)\n",
    "                a_dets.append(a_kws)\n",
    "                article_list.append(a_dets)\n",
    "        except:\n",
    "            problem_DOIs.append(doi)\n",
    "            \n",
    "        if progress % 50 == 0:\n",
    "            print(progress)\n",
    "\n",
    "\n",
    "j_data = pd.DataFrame(article_list)\n",
    "j_data.columns = [\n",
    "    \"DOI\",\n",
    "    \"AUTHOR_NAME\",\n",
    "    \"AUTHOR_ORDER\",\n",
    "    \"AFFILIATION\",\n",
    "    \"TITLE\",\n",
    "    \"DATE\",\n",
    "    \"VOLUME\",\n",
    "    \"ISSUE\",\n",
    "    \"PAGES\",\n",
    "    \"KEYWORDS\"\n",
    "]\n",
    "\n",
    "print(\"Number of Problem DOIS: \", len(problem_DOIs))\n",
    "\n",
    "with open(TITLE+\"/\"+ISSN+\"_BAD_DOI.txt\",\"w\") as b_doi:\n",
    "    for d in problem_DOIs:\n",
    "        b_doi.write(d+\"\\n\")\n",
    "\n",
    "#CSV of progress thus far\n",
    "j_data.to_csv(TITLE+\"/\"+TITLE+\"_\"+ISSN+\".csv\",index=False)\n",
    "print(\"...Done.\")\n",
    "\n",
    "\n",
    "#### Stage 3\n",
    "print(\"\\nSTAGE 3: Download HTML Landing Pages\")\n",
    "counter = 0\n",
    "with open(TITLE+\"/\"+ISSN+\"_DOI.txt\") as doi_file:\n",
    "    for d in doi_file:\n",
    "        counter += 1\n",
    "        url_to_grab = \"https://www.tandfonline.com/doi/full/\"+d.strip(\"\\n\")\n",
    "        label = d.replace(\"/\",\"_\")\n",
    "        html = requests.get(url_to_grab).text\n",
    "        if not glob.glob(TITLE+\"/\"+label+\".html\"): #Only download if we didn't already\n",
    "            try:\n",
    "                html_grabbed = open(TITLE+\"/\"+label+\".html\",\"w\")\n",
    "                html_grabbed.write(html)\n",
    "            except:\n",
    "                print(\"Problem with: \",d)\n",
    "        if counter % 50 == 0:\n",
    "            print(counter)\n",
    "            \n",
    "print(\"...Done.\")\n",
    "\n",
    "#### Stage 4\n",
    "print(\"\\nSTAGE 4: Enrich Dataframe with HTML Data\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "art_list_md = []\n",
    "counter = 0\n",
    "#open all html files and build into Dataframe\n",
    "#for html_md in glob.glob(TITLE+\"/*.html\"):\n",
    "\n",
    "#for html_md in glob.glob(TITLE+\"/10.1080_16184742.2010.502741?.html\"):\n",
    "for html_md in glob.glob(TITLE+\"/*.html\"):\n",
    "    counter +=1\n",
    "    with open(html_md) as h_file:\n",
    "        \n",
    "        a_md = []\n",
    "        soup = BeautifulSoup(h_file,'html.parser')\n",
    "        \n",
    "        #DOI of this item\n",
    "        try:\n",
    "            a_doi = soup.find(\"li\",{\"class\":\"dx-doi\"}).text.split(\"/\",3)[3]\n",
    "            a_md.append(a_doi)\n",
    "        except:\n",
    "            a_doi = \"NA\"\n",
    "        \n",
    "        #Abstract\n",
    "        try:\n",
    "            a_abs = soup.find(\"div\",{\"class\":\"abstractSection abstractInFull\"}).text\n",
    "        except:\n",
    "            a_abs = \"NA\"\n",
    "        a_md.append(a_abs)\n",
    "    \n",
    "    art_list_md.append(a_md)\n",
    "    if counter % 50 == 0:\n",
    "        print(counter)\n",
    "    \n",
    "j_extra_md = pd.DataFrame(art_list_md)\n",
    "j_extra_md.columns = [\"DOI\",\"ABSTRACT\"]\n",
    "    \n",
    "final_df = j_data.merge(j_extra_md, left_on=\"DOI\",right_on=\"DOI\")\n",
    "final_df.to_csv(TITLE+\"/\"+TITLE+\"_\"+ISSN+\".csv\",index=False)\n",
    "\n",
    "print(\"...Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
